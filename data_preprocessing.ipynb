{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from os.path import expanduser, join, basename, dirname\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from shutil import copy\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from albk.data.utils import idx_to_locate\n",
    "use_disjoint_files = False\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from glob import glob\n",
    "from os.path import expanduser, join, basename, dirname\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_label_files(path1, path2):\n",
    "    files1 = glob(join(path1, \"*.nc\"))\n",
    "    files2 = glob(join(path2, \"*.nc\"))\n",
    "    \n",
    "    f1_base_files = [basename(f) for f in files1]\n",
    "    f2_base_files = [basename(f) for f in files2]\n",
    "    \n",
    "    common_files = set(f1_base_files).intersection(f2_base_files)\n",
    "    common_label_files = []\n",
    "    for file in common_files:\n",
    "        ds1 = xr.open_dataset(join(path1, file))\n",
    "        ds2 = xr.open_dataset(join(path2, file))\n",
    "        if np.all(ds1.label.values == ds2.label.values):\n",
    "            common_label_files.append(file)\n",
    "    \n",
    "    return list(map(lambda f: join(path1, f), common_label_files))\n",
    "\n",
    "def get_disjoint_files(path1, path2):\n",
    "    files1 = glob(join(path1, \"*.nc\"))\n",
    "    files2 = glob(join(path2, \"*.nc\"))\n",
    "    \n",
    "    f1_base_files = [basename(f) for f in files1]\n",
    "    f2_base_files = [basename(f) for f in files2]\n",
    "    \n",
    "    disjoint_files = set(f1_base_files).symmetric_difference(f2_base_files)\n",
    "    \n",
    "    f1_disjoint = [f for f in disjoint_files if f in f1_base_files]\n",
    "    f1_disjoint = list(map(lambda f: join(path1, f), f1_disjoint))\n",
    "\n",
    "    f2_disjoint = [f for f in disjoint_files if f in f2_base_files]\n",
    "    f2_disjoint = list(map(lambda f: join(path2, f), f2_disjoint))\n",
    "    \n",
    "    return f1_disjoint + f2_disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moderator zeel\n",
      "      Moderator files 88\n",
      "      Common label files 359\n",
      "      Disjoint files 662\n",
      "      Total files from zeel and ('vannsh', 'rishabh') 447\n",
      "      Total annotatated files 447\n",
      "Moderator rishabh\n",
      "      Moderator files 98\n",
      "      Common label files 115\n",
      "      Disjoint files 736\n",
      "      Total files from rishabh and ('suraj', 'dhruv') 213\n",
      "      Total annotatated files 660\n",
      "Moderator suraj\n",
      "      Moderator files 195\n",
      "      Common label files 165\n",
      "      Disjoint files 746\n",
      "      Total files from suraj and ('aditi', 'madhav') 360\n",
      "      Total annotatated files 1020\n",
      "Total dataset size 25500\n"
     ]
    }
   ],
   "source": [
    "base_path = expanduser(\"~/bangladesh_labels/bkdb/bangladesh_labels\")\n",
    "paths = {\"zeel\": (\"vannsh\", \"rishabh\"), \"rishabh\": (\"suraj\", \"dhruv\"), \"suraj\": (\"aditi\", \"madhav\")}\n",
    "\n",
    "all_labeled_files = []\n",
    "for moderator, annotators in paths.items():\n",
    "    # Get moderator files\n",
    "    moderator_path = join(base_path, \"moderated\", moderator)\n",
    "    moderator_files = glob(join(moderator_path, \"*.nc\"))\n",
    "    \n",
    "    # Get annotator common label files\n",
    "    annotator1_path = join(base_path, annotators[0])\n",
    "    annotator2_path = join(base_path, annotators[1])\n",
    "    \n",
    "    common_base_files = get_common_label_files(annotator1_path, annotator2_path)\n",
    "    \n",
    "    # Get disjoint files\n",
    "    disjoint_files = get_disjoint_files(annotator1_path, annotator2_path)\n",
    "    \n",
    "    all_files = moderator_files + common_base_files\n",
    "    if use_disjoint_files:\n",
    "        all_files.extend(disjoint_files)\n",
    "    assert len(all_files) == len(set(all_files))\n",
    "    all_labeled_files.extend(all_files)\n",
    "    \n",
    "    print(\"Moderator\", moderator)\n",
    "    print(\" \"*5, \"Moderator files\", len(moderator_files))\n",
    "    print(\" \"*5, \"Common label files\", len(common_base_files))\n",
    "    print(\" \"*5, \"Disjoint files\", len(disjoint_files))\n",
    "    print(\" \"*5, f\"Total files from {moderator} and {annotators}\", len(all_files))\n",
    "    print(\" \"*5, \"Total annotatated files\", len(all_labeled_files))\n",
    "    \n",
    "print(\"Total dataset size\", len(all_labeled_files) * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/rishabh.mondal/bangladesh_labels/bkdb/bangladesh_labels/moderated/zeel/24.90,90.77.nc', '/home/rishabh.mondal/bangladesh_labels/bkdb/bangladesh_labels/moderated/zeel/24.58,91.69.nc', '/home/rishabh.mondal/bangladesh_labels/bkdb/bangladesh_labels/moderated/zeel/24.44,90.83.nc', '/home/rishabh.mondal/bangladesh_labels/bkdb/bangladesh_labels/moderated/zeel/24.63,88.25.nc', '/home/rishabh.mondal/bangladesh_labels/bkdb/bangladesh_labels/moderated/zeel/24.99,89.79.nc']\n"
     ]
    }
   ],
   "source": [
    "print(all_labeled_files[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ZicZac Brick Kilns 1285\n",
      "Total Fixed Chimney Brick Kilns 412\n",
      "All Brick Kilns 1697\n",
      "All Non-brick Kilns 23803\n"
     ]
    }
   ],
   "source": [
    "def get_bk_stats(path):\n",
    "    ds = xr.open_dataset(path)\n",
    "    z = (ds.label.values == \"Z\").sum()\n",
    "    f = (ds.label.values == \"F\").sum()\n",
    "    o = (ds.label.values == \"O\").sum()\n",
    "    return {\"Z\": z, \"F\": f, \"O\": o}\n",
    "\n",
    "df = pd.DataFrame([get_bk_stats(path) for path in all_labeled_files])\n",
    "\n",
    "df_sum = df.sum(axis=0)\n",
    "print(\"Total ZicZac Brick Kilns\", df_sum[\"Z\"])\n",
    "print(\"Total Fixed Chimney Brick Kilns\", df_sum[\"F\"])\n",
    "print(\"All Brick Kilns\", df_sum[\"Z\"] + df_sum[\"F\"])\n",
    "print(\"All Non-brick Kilns\", df_sum[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = expanduser(\"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/bangladesh_labels/\")\n",
    "# os.system(f\"rm -rf {save_path}\")\n",
    "# os.makedirs(save_path)\n",
    "\n",
    "# def copy_file(path):\n",
    "#     copy(path, save_path)\n",
    "    \n",
    "# _ = Parallel(n_jobs=20)(delayed(copy_file)(path) for path in tqdm(all_labeled_files))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020\n"
     ]
    }
   ],
   "source": [
    "images_path = expanduser(\"/home/patel_zeel/bkdb/bangladesh/\")\n",
    "# load_path = \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/temporary\"\n",
    "files = all_labeled_files\n",
    "# print(files)\n",
    "print(len(files))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_and_image(file):\n",
    "    index = []\n",
    "    images = []\n",
    "    labels = []\n",
    "    base_name = basename(file)\n",
    "    # print(base_name)\n",
    "    image_path = join(images_path, base_name).replace(\".nc\", \".zarr\")\n",
    "    print(image_path)\n",
    "    # print()\n",
    "    label_ds = xr.open_dataset(file)\n",
    "    # print (label_ds)\n",
    "    image_ds = xr.open_zarr(image_path, consolidated=False)\n",
    "    # image = image_ds.data.reshape(-1, 224, 224, 3)\n",
    "    for lat_lag, lon_lag in product(range(-2, 3), repeat=2):\n",
    "        index.append(base_name.replace(\".nc\", \"\")+f\"_{lat_lag}_{lon_lag}\")\n",
    "        images.append(torch.tensor(image_ds.sel(lat_lag=lat_lag, lon_lag=lon_lag)['data'].values)[np.newaxis, ...])\n",
    "        labels.append(torch.tensor((label_ds.sel(lat_lag=lat_lag, lon_lag=lon_lag)['label'].values != \"O\").astype(np.uint8)))\n",
    "        \n",
    "    return index, images, labels\n",
    "\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    out = Parallel(n_jobs=32)(delayed(get_index_and_image)(file) for file in tqdm(files, total=len(files)))\n",
    "    index = np.concatenate([np.array(idx) for idx, _, _ in out])\n",
    "    images = torch.concat([torch.einsum(\"nhwc->nchw\", torch.concat(imgs)) for _, imgs, _ in out])\n",
    "    # scale\n",
    "    # images = images / 255\n",
    "    # mean normalize\n",
    "    # images = (images - images.mean(dim=(0, 2, 3), keepdim=True)) / images.std(dim=(0, 2, 3), keepdim=True)\n",
    "    \n",
    "    labels = np.concatenate([np.array(lbl) for _, _, lbl in out])\n",
    "    labels = torch.tensor(labels, dtype=torch.uint8)\n",
    "    #check the all dytpes\n",
    "    print(index.dtype, images.dtype, labels.dtype)\n",
    "    return index, images, labels\n",
    "\n",
    "index, images, labels = get_data()\n",
    "print(images.dtype)\n",
    "print(index.shape, images.shape, labels.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.uint8\n",
      "torch.uint8\n"
     ]
    }
   ],
   "source": [
    "print(images.dtype)\n",
    "print(labels.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dont run the below cell otherwise it will overwrite the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tensors data \n",
    "# print(images.dtype)\n",
    "# save_path=\"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/tensor_data/data.pt\"\n",
    "# torch.save({\n",
    "#     'index': index,\n",
    "#     'images': images,\n",
    "#     'labels': labels\n",
    "# }, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can run the below cell if you want to access the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved tensors\n",
    "loaded_data = torch.load(\"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/tensor_data/test_data.pt\")\n",
    "\n",
    "# Access the tensors\n",
    "index = loaded_data['index']\n",
    "images = loaded_data['images']\n",
    "labels = loaded_data['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10025,) torch.Size([10025, 3, 224, 224]) torch.Size([10025])\n"
     ]
    }
   ],
   "source": [
    "print(index.shape, images.shape, labels.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.uint8, torch.uint8, dtype('<U17'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.dtype, labels.dtype, index.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For mannual splitting of data into train and test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Cross Validation spliting of data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 6737, 1: 781})\n",
      "Counter({0: 2246, 1: 261})\n",
      "Fold 1 - Train: Counter({0: 6737, 1: 781}), Test: Counter({0: 2246, 1: 261})\n",
      "Counter({0: 6737, 1: 782})\n",
      "Counter({0: 2246, 1: 260})\n",
      "Fold 2 - Train: Counter({0: 6737, 1: 782}), Test: Counter({0: 2246, 1: 260})\n",
      "Counter({0: 6737, 1: 782})\n",
      "Counter({0: 2246, 1: 260})\n",
      "Fold 3 - Train: Counter({0: 6737, 1: 782}), Test: Counter({0: 2246, 1: 260})\n",
      "Counter({0: 6738, 1: 781})\n",
      "Counter({0: 2245, 1: 261})\n",
      "Fold 4 - Train: Counter({0: 6738, 1: 781}), Test: Counter({0: 2245, 1: 261})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "fold_data = []  # List to store data from each fold\n",
    "\n",
    "seed = 42  # Use your desired random seed\n",
    "splitter = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "images = images / 255\n",
    "    # mean normalize\n",
    "images = (images - images.mean(dim=(0, 2, 3), keepdim=True)) / images.std(dim=(0, 2, 3), keepdim=True)\n",
    "for fold, (train_idx, test_idx) in enumerate(splitter.split(images, labels)):\n",
    "    X_train, X_test = images[train_idx], images[test_idx]\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "    # Count occurrences of each class in train and test sets\n",
    "    train_counter = Counter(y_train.numpy())\n",
    "    test_counter = Counter(y_test.numpy())\n",
    "    print(train_counter)\n",
    "    print(test_counter)\n",
    "    print(f\"Fold {fold + 1} - Train: {train_counter}, Test: {test_counter}\")\n",
    "\n",
    "    fold_data.append({\n",
    "        'fold': fold + 1,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'train_counter': train_counter,\n",
    "        'test_counter': test_counter\n",
    "    })\n",
    "\n",
    "# Save the list of fold data using torch.save\n",
    "# torch.save(fold_data, 'f/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/cross_val_data/fold_data.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_space",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
